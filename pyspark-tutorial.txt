				***Introdution***

Apache Spark is a lightning fast real-time processing framework. 
It does in-memory computations to analyze data in real-time. 
It came into picture as Apache Hadoop MapReduce was performing batch processing only and lacked a real-time processing feature. 
Hence, Apache Spark was introduced as it can perform stream processing in real-time and can also take care of batch processing.

Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark Community released a tool, PySpark. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this.

PySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context. Majority of data scientists and analytics experts today use Python because of its rich library set. Integrating Python with Spark is a boon to them.

SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes.

SparkContext uses Py4J to launch a JVM and creates a JavaSparkContext. By default, PySpark has SparkContext available as ‘sc’, so creating a new SparkContext won't work.

				***RDD***

RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster. RDDs are immutable elements(create RDD,cannot change it)
RDDs are fault tolerant(in case of any failure, they recover automatically)
You can apply mutliple operations on these RDDs to achieve a certain task.

To apply operations on these RDD's, there are two ways - 
	-- Transformation
	-- Action
Transformation - these are the operations, which are applied on a RDD to create a new RDD.
(for example filter, groupBy, map)
Action - these are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver.

				***Broadcast&accumulator***

For parallel processing, Apache Spark uses shared variables. A copy of shared variable goes on each node of the cluster when the driver sends a task to the executor on the cluster, so that it can be used for performing tasks.
There are two types of shared variables supported by Apache Spark -
	-- Broadcast
	-- Accumulator
Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks.

Accumulator variables are used for aggregating the information through aasociative and commutative operations. For example, we can use an accumulator for a sum ooperation or counters.
An Accumulator variable has an attribute called value that is similar to what a broadcast variable has. It stores the data used to return the accumulator's value, but usable only in a driver program.











